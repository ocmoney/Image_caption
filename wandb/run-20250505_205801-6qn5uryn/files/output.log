Processing dataset...
Processing train split...
Taking 29 examples (0.1% of train split)...
Map:   0%|                                                                    | 0/145 [00:05<?, ? examples/s]
image_embeddings shape: torch.Size([8, 196, 768])
sos_embedding shape: torch.Size([8, 1, 768])
caption_embeddings shape: torch.Size([8, 32, 768])
Traceback (most recent call last):
  File "c:\Users\ollie\OneDrive - University of Bristol\Documents\coding\MLIWeek4\upload_to_wandb.py", line 41, in <module>
    upload_dataset()
  File "c:\Users\ollie\OneDrive - University of Bristol\Documents\coding\MLIWeek4\upload_to_wandb.py", line 13, in upload_dataset
    processed_dataset = process_dataset()
                        ^^^^^^^^^^^^^^^^^
  File "c:\Users\ollie\OneDrive - University of Bristol\Documents\coding\MLIWeek4\img_text_concat.py", line 179, in process_dataset
    processed_splits['train'] = expanded_dataset.map(
                                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ollie\Anaconda3\envs\Institue\Lib\site-packages\datasets\arrow_dataset.py", line 557, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ollie\Anaconda3\envs\Institue\Lib\site-packages\datasets\arrow_dataset.py", line 3074, in map
    for rank, done, content in Dataset._map_single(**dataset_kwargs):
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ollie\Anaconda3\envs\Institue\Lib\site-packages\datasets\arrow_dataset.py", line 3531, in _map_single
    writer.write_batch(batch)
  File "C:\Users\ollie\Anaconda3\envs\Institue\Lib\site-packages\datasets\arrow_writer.py", line 606, in write_batch
    arrays.append(pa.array(typed_sequence))
                  ^^^^^^^^^^^^^^^^^^^^^^^^
  File "pyarrow\\array.pxi", line 252, in pyarrow.lib.array
  File "pyarrow\\array.pxi", line 114, in pyarrow.lib._handle_arrow_array_protocol
  File "C:\Users\ollie\Anaconda3\envs\Institue\Lib\site-packages\datasets\arrow_writer.py", line 219, in __arrow_array__
    storage = to_pyarrow_listarray(data, pa_type)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ollie\Anaconda3\envs\Institue\Lib\site-packages\datasets\features\features.py", line 1594, in to_pyarrow_listarray
    return pa.array(data, pa_type.storage_dtype)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "pyarrow\\array.pxi", line 372, in pyarrow.lib.array
  File "pyarrow\\array.pxi", line 42, in pyarrow.lib._sequence_to_array
  File "pyarrow\\error.pxi", line 155, in pyarrow.lib.pyarrow_internal_check_status
  File "pyarrow\\error.pxi", line 92, in pyarrow.lib.check_status
pyarrow.lib.ArrowTypeError: Could not convert tensor([[ 0.3303,  0.0722, -0.1909,  ...,  0.0163,  0.0606,  0.1517],
        [ 0.3303,  0.0722, -0.1909,  ...,  0.0163,  0.0606,  0.1517],
        [ 0.3303,  0.0722, -0.1909,  ...,  0.0163,  0.0606,  0.1517],
        ...,
        [ 0.3310,  0.0717, -0.1860,  ...,  0.0246,  0.0595,  0.1542],
        [ 0.3310,  0.0717, -0.1860,  ...,  0.0246,  0.0595,  0.1542],
        [ 0.3310,  0.0717, -0.1860,  ...,  0.0246,  0.0595,  0.1542]],
       grad_fn=<UnbindBackward0>) with type Tensor: was not a sequence or recognized null for conversion to list type
